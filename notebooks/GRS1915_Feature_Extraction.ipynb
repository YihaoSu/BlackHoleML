{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Our data comes in the form of observations done in X-rays over a course of 16 years.\n",
    "Each observation is of the order of few thousand seconds long (sometimes less, rarely more) of \n",
    "uninterrupted data. For each unterrupted time interval in which data was taken, we have **time series**\n",
    "with a 0.125 second time resolution, taken in four different **energy bands**. \n",
    "Additionally, in each observation, the source could have been in one state continuously, but it also \n",
    "could have changed state in the middle. GRS 1915+105 is known to switch between two states several times\n",
    "on occasion before settling into a new state. \n",
    "\n",
    "In summary, our data consists of four simultaneous (correlated, but not necessarily identical) time series.\n",
    "Our task is to extract meaningful **features** for these time series that will capture the behaviour of \n",
    "the source and the different states it goes through such that they will be recognizable by a machine learning\n",
    "algorithm. \n",
    "\n",
    "With the exception of a few deep learning methods (which we will not touch here!), we cannot simply put our \n",
    "data into a classifier directly, for several reasons:\n",
    "- in order to capture the behaviour seen by human classifiers accurately, we need to feed long light curves into the classifier; for 1024s time series, that translates to 1024x4x4 samples, at which point dimensionality is a definite problem: *we need to extract features that will summarize the behaviour in as few numbers as possible*\n",
    "- even if our time series are longer than the typical behaviour of the source, machine learning classifiers are not phase invariant: if I have two time series of a perfectly sinusoidal signal that are out of phase with each other, the classifier will consider them drastically different, because it compares sample by samples (time bin by time bin): *this means we need to extract features that capture the behaviour of the time series in a phase-invariant way*\n",
    "- there is behaviour in the time series that may be better captures by an alternative representation: *in particular, quasi-periodic features may be better captured in the Fourier domain than in the time domain*\n",
    "- GRS1915+105 exhibits clear energy-dependent behaviour over time; that is, the energy spectrum of the source changes both within states and between states. *We need to extract features that describe this energy-dependent behaviour*\n",
    "\n",
    "### A set of features to explore\n",
    "\n",
    "We will extract features from three domains and will later explore how predictive they are on supervised machine learning tasks. Features will summarize properties of the four simultaneous time series in three domains: the time domain, the Fourier domain, and the energy domain.\n",
    "\n",
    "Time domain features are:\n",
    "- mean count rate `fmu`\n",
    "- variance in the total light curve `fvar`\n",
    "\n",
    "Fourier domain features are:\n",
    "- integrated power in 4 bands: \n",
    "    - PA: 0.0039-0.031 Hz, \n",
    "    - PB: 0.031-0.25 Hz, \n",
    "    - PC: 0.25-2.0 Hz and \n",
    "    - PD: 2.0-16.0 Hz\n",
    "- power colours: PC/PA and PB/PD\n",
    "- frequency at which maximum power is observed\n",
    "\n",
    "\n",
    "Energy domain features are:\n",
    "- mean and covariance of hardness ratios in 2 bands (processed data only, NOT Standard 1 data)\n",
    "    - (2-6keV)/(2-13keV)\n",
    "    - (9-20kev)/(2-13keV) \n",
    "\n",
    "\n",
    "\n",
    "First, we'll need to load the data. This assumes that you've saved a pickle file with a list of $N$ light curves and states, `[[lc1, state1], [lc2, state2], ..., [lc_n, state_n]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import powerspectrum\n",
    "\n",
    "sys.path.append(\"/Users/danielahuppenkothen/work/repositories/LightEcho/code/\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import linearfilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file contains the prepared light curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"../../grs1915_125ms_clean.dat\")\n",
    "d_all = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the frequencies of states in these light curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of light curves:  2829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chi1      100\n",
       "chi2       68\n",
       "rho        53\n",
       "theta      48\n",
       "chi4       47\n",
       "beta       39\n",
       "delta      39\n",
       "kappa      38\n",
       "gamma      36\n",
       "phi        24\n",
       "mu         17\n",
       "nu         17\n",
       "chi3       12\n",
       "alpha      10\n",
       "lambda      9\n",
       "eta         8\n",
       "omega       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of light curves:  \" + str(len(d_all)))\n",
    "states = [d[1] for d in d_all]\n",
    "st = pd.Series(states)\n",
    "st.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the classes are *fairly* evenly distributed, with $\\chi_2$ having the most light curves in their samples, and $\\eta$ and $\\omega$ being severely undersampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_labelled = [d for d in d_all if d[1] != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total number of 2829 light curves.\n",
      "571 of them are labelled. \n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total number of %i light curves.\"%len(d_all))\n",
    "print(\"%i of them are labelled. \"%len(d_labelled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we'll pick out training set, validation set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1697 light curves in the training set.\n",
      "There are 566 light curves in the validation set.\n",
      "There are 566 light curves in the test set.\n",
      "These is the distribution of states in the training set: \n",
      "chi1      59\n",
      "chi2      37\n",
      "rho       34\n",
      "chi4      33\n",
      "theta     29\n",
      "delta     26\n",
      "kappa     25\n",
      "gamma     23\n",
      "beta      20\n",
      "phi       14\n",
      "mu        12\n",
      "nu        11\n",
      "alpha      6\n",
      "chi3       6\n",
      "lambda     6\n",
      "eta        4\n",
      "omega      3\n",
      "dtype: int64\n",
      "================================================================\n",
      "These is the distribution of states in the validation set: \n",
      "chi1      19\n",
      "chi2      15\n",
      "theta     11\n",
      "chi4       8\n",
      "beta       7\n",
      "kappa      7\n",
      "rho        7\n",
      "gamma      5\n",
      "delta      5\n",
      "phi        4\n",
      "chi3       3\n",
      "lambda     3\n",
      "eta        3\n",
      "nu         3\n",
      "omega      2\n",
      "mu         2\n",
      "alpha      1\n",
      "dtype: int64\n",
      "================================================================\n",
      "These is the distribution of states in the test set: \n",
      "chi1     22\n",
      "chi2     16\n",
      "rho      12\n",
      "beta     12\n",
      "gamma     8\n",
      "theta     8\n",
      "delta     8\n",
      "kappa     6\n",
      "chi4      6\n",
      "phi       6\n",
      "nu        3\n",
      "mu        3\n",
      "alpha     3\n",
      "chi3      3\n",
      "eta       1\n",
      "omega     1\n",
      "dtype: int64\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "## total number of light curves\n",
    "n_lcs = len(d_all)\n",
    "\n",
    "## Set the seed to I will always pick out the same light curves.\n",
    "np.random.seed(20160615)\n",
    "\n",
    "## shuffle list of light curves\n",
    "np.random.shuffle(d_all)\n",
    "\n",
    "train_frac = 0.6\n",
    "validation_frac = 0.2\n",
    "test_frac = 0.2\n",
    "\n",
    "## let's pull out light curves for three data sets into different variables.\n",
    "d_all_train = d_all[:int(train_frac*n_lcs)]\n",
    "d_all_val = d_all[int(train_frac*n_lcs):int((train_frac + validation_frac)*n_lcs)]\n",
    "d_all_test = d_all[int((train_frac + validation_frac)*n_lcs):]\n",
    "\n",
    "## Let's print some information about the three sets.\n",
    "print(\"There are %i light curves in the training set.\"%len(d_all_train))\n",
    "print(\"There are %i light curves in the validation set.\"%len(d_all_val))\n",
    "print(\"There are %i light curves in the test set.\"%len(d_all_test))\n",
    "for da,n in zip([d_all_train, d_all_val, d_all_test], [\"training\", \"validation\", \"test\"]):\n",
    "    print(\"These is the distribution of states in the %s set: \"%n)\n",
    "    states = [d[1] for d in da]\n",
    "    st = pd.Series(states)\n",
    "    print(st.value_counts())\n",
    "    print(\"================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we can make light curve segments, overlapping or not, to then pass to feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## This function is also in grs1915_utils.py!\n",
    "def extract_segments(d_all, seg_length = 256., overlap=128., dt=0.125):\n",
    "    \"\"\" Extract light curve segmens from a list of light curves. \n",
    "        Each element in the list is a list with two elements: \n",
    "        - an array that contains the light curve in three energy bands \n",
    "        (full, low energies, high energies) and \n",
    "        - a string containing the state of that light curve.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        seg_length : float\n",
    "            The length of each segment. Bits of data at the end of a light curve\n",
    "            that are smaller than seg_length will not be included. \n",
    "        \n",
    "        overlap : float \n",
    "            This is actually the interval between start times of individual segments,\n",
    "            i.e. by default light curves start 64 seconds apart. The actual overlap is \n",
    "            seg_length-overlap\n",
    "            \n",
    "        dt : float\n",
    "            Set the time resolution of the light curve by hand. I allow this because \n",
    "            in the GRS 1915+105 data, there are rounding errors affecting the time \n",
    "            resolution, which in turn affects the length of the segments. \n",
    "    \"\"\"\n",
    "    segments, labels = [], [] ## labels for labelled data    \n",
    "    \n",
    "    for i,d_seg in enumerate(d_all):\n",
    "        \n",
    "        ## data is an array in the first element of d_seg\n",
    "        data = d_seg[0]\n",
    "        ## state is a string in the second element of d_seg\n",
    "        state = d_seg[1]\n",
    "        \n",
    "        ## if the light curve is shorter than the segment length,\n",
    "        ## throw out!\n",
    "        length = data[-1,0] - data[0,0]\n",
    "        if length < seg_length:\n",
    "            continue\n",
    "            \n",
    "        ## compute the number of time bins in a segment\n",
    "        nseg = seg_length/dt\n",
    "        ## compute the number of time bins to start of next segment\n",
    "        noverlap = overlap/dt\n",
    "        \n",
    "        istart = 0\n",
    "        iend = nseg\n",
    "        j = 0\n",
    "     \n",
    "        while iend <= len(data):\n",
    "            dtemp = data[istart:iend]\n",
    "            segments.append(dtemp)\n",
    "            labels.append(state)\n",
    "            istart += noverlap\n",
    "            iend += noverlap\n",
    "            j+=1\n",
    "        \n",
    "    return segments, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll extract 1024 second long segments for the supervised classification.\n",
    "\n",
    "For the supervised classification, we would like to capture as much of the states as observed by Belloni et al in  each light curve (such that there isn't too much variance between features of samples within a class), but we also need to retain enough light curves for a useful training set. \n",
    "\n",
    "We can choose some overlap between segments to augment our data set and allow the classifier to \"see\" different parts of the same light curve shape. Here, we choose 128 seconds for all further analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10164 segments in the training set.\n",
      "There are 3641 segments in the validation set.\n",
      "There are 3625 segments in the test set.\n",
      "These is the distribution of states in the training set: \n",
      "chi1      320\n",
      "chi2      262\n",
      "chi4      205\n",
      "theta     197\n",
      "gamma     191\n",
      "rho       184\n",
      "phi       153\n",
      "kappa     126\n",
      "beta      108\n",
      "mu        104\n",
      "delta      68\n",
      "nu         43\n",
      "lambda     42\n",
      "alpha      35\n",
      "eta        29\n",
      "chi3       22\n",
      "omega      10\n",
      "dtype: int64\n",
      "================================================================\n",
      "These is the distribution of states in the validation set: \n",
      "chi1      121\n",
      "chi2      118\n",
      "theta      80\n",
      "chi4       62\n",
      "beta       50\n",
      "chi3       42\n",
      "eta        42\n",
      "gamma      36\n",
      "lambda     25\n",
      "nu         21\n",
      "omega      18\n",
      "rho        18\n",
      "phi        18\n",
      "mu         18\n",
      "kappa      17\n",
      "delta       3\n",
      "dtype: int64\n",
      "================================================================\n",
      "These is the distribution of states in the test set: \n",
      "chi2     138\n",
      "beta     106\n",
      "chi1     101\n",
      "rho       95\n",
      "theta     84\n",
      "gamma     65\n",
      "chi4      36\n",
      "kappa     36\n",
      "chi3      35\n",
      "phi       32\n",
      "delta     26\n",
      "alpha     18\n",
      "eta       17\n",
      "nu        13\n",
      "omega      3\n",
      "dtype: int64\n",
      "================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/daniela/sw/miniconda/lib/python2.7/site-packages/ipykernel/__main__.py:52: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "seg_length_supervised = 1024.\n",
    "\n",
    "overlap_all = 128.\n",
    "dt = 0.125\n",
    "\n",
    "## CURRENTLY RUNNING LONG SEGMENTS FOR SUPERVISED CLASSIFICATION\n",
    "seg_train, labels_train = extract_segments(d_all_train, seg_length=seg_length_supervised, \n",
    "                                           overlap=overlap_all, dt=0.125)\n",
    "seg_val, labels_val = extract_segments(d_all_val, seg_length=seg_length_supervised, \n",
    "                                       overlap=overlap_all, dt=0.125)\n",
    "seg_test, labels_test = extract_segments(d_all_test, seg_length=seg_length_supervised, \n",
    "                                         overlap=overlap_all, dt=0.125)\n",
    "\n",
    "## Let's print some details on the different segment data sets\n",
    "print(\"There are %i segments in the training set.\"%len(seg_train))\n",
    "print(\"There are %i segments in the validation set.\"%len(seg_val))\n",
    "print(\"There are %i segments in the test set.\"%len(seg_test))\n",
    "for la,n in zip([labels_train, labels_val, labels_test], [\"training\", \"validation\", \"test\"]):\n",
    "    print(\"These is the distribution of states in the %s set: \"%n)\n",
    "    st = pd.Series(la)\n",
    "    print(st.value_counts())\n",
    "    print(\"================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can think about actual feature extraction.\n",
    "\n",
    "Let's make some functions to extract individual features from a segment. We can then combine them at will. \n",
    "\n",
    "### Time Series Features\n",
    "\n",
    "First, we're going to extract time series features derived from the light curve:\n",
    "\n",
    "- mean, \n",
    "- median \n",
    "- variance \n",
    "- skewness\n",
    "- kurtosis\n",
    "- weights from a linear filter \n",
    "\n",
    "The linear filter requires two hyperparameters:\n",
    "1. the number of previous time bins to use in the determination of the current flux; this translates directly into the number of features added (i.e. $k=10$ will translate into 10 features added by this representation of the data)\n",
    "2. the regularization term for the ridge regression (which can also be determined via cross validation)\n",
    "\n",
    "We will determine the linear filter hyperparameters in the feature engineering step of the analysis.\n",
    "\n",
    "**NOTE: The linear filter routine takes *all* light curves at the same time! Additionally, we'll want these \n",
    "light curves all scaled to the same mean and variance before applying the filter!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### extract summary statistics from the time series\n",
    "def timeseries_features(seg):\n",
    "    counts = seg[:,1] ## extract counts in full band\n",
    "    fmean = np.mean(counts)                    ## mean\n",
    "    fmedian = np.median(counts)                ## median\n",
    "    fvar = np.var(counts)                      ## variance\n",
    "    skew = scipy.stats.skew(counts)            ## skewness\n",
    "    kurt = scipy.stats.kurtosis(counts)        ## kurtosis\n",
    "    return fmean, fmedian, fvar, skew, kurt\n",
    "\n",
    "\n",
    "def linear_filter(counts_scaled, k=10):\n",
    "    \"\"\"\n",
    "    Extract features from a linear filter. This is actually an autoregressive model!\n",
    "    :param counts_scaled: numpy ndarray (nsamples, ntimebins) with SCALED TOTAL COUNTS\n",
    "    :return ww_all: numpy ndarray (nsamples, k) of weights \n",
    "    \"\"\"\n",
    "  \n",
    "    ## initialize the LinearFilter object\n",
    "    lf = linearfilter.LinearFilter(k=k)\n",
    "    \n",
    "    ww_all = np.zeros((len(counts_scaled), k))\n",
    "    \n",
    "    ## loop through all light curves and compute the weight vector for each\n",
    "    for i,c in enumerate(counts_scaled):\n",
    "        ww_all[i,:] = lf.fit_transform(c)[0]\n",
    "    return ww_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Domain Features\n",
    "\n",
    "We'll extract the integrated power in four power spectral bands, as well as power colours (ratios of these integrated PSD bands) and the maximum frequency at which we observe the maximum power (as a proxy for the presence of a QPO). \n",
    "The power spectral bands are taken from Lucy's paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## boundaries for power spectral bands, from Heil et al, 2014\n",
    "pcb = {\"pa_min\":0.0039, \"pa_max\":0.031,\n",
    "       \"pb_min\":0.031, \"pb_max\":0.25,\n",
    "       \"pc_min\":0.25, \"pc_max\":2.0,\n",
    "       \"pd_min\":2.0, \"pd_max\":16.0}\n",
    "\n",
    "def rebin_psd(freq, ps, n=10, type='average'):\n",
    "\n",
    "    nbins = int(len(freq)/n)\n",
    "    df = freq[1] - freq[0]\n",
    "    T = freq[-1] - freq[0] + df\n",
    "    bin_df = df*n\n",
    "    binfreq = np.arange(nbins)*bin_df + bin_df/2.0 + freq[0]\n",
    "\n",
    "    #print(\"len(ps): \" + str(len(ps)))\n",
    "    #print(\"n: \" + str(n))\n",
    "\n",
    "    nbins_new = int(len(ps)/n)\n",
    "    ps_new = ps[:nbins_new*n]\n",
    "    binps = np.reshape(np.array(ps_new), (nbins_new, int(n)))\n",
    "    binps = np.sum(binps, axis=1)\n",
    "    if type in [\"average\", \"mean\"]:\n",
    "        binps = binps/np.float(n)\n",
    "    else:\n",
    "        binps = binps\n",
    "\n",
    "    if len(binfreq) < len(binps):\n",
    "        binps= binps[:len(binfreq)]\n",
    "\n",
    "    return binfreq, binps\n",
    "\n",
    "\n",
    "def psd_features(seg, pcb):\n",
    "    \"\"\"\n",
    "    Computer PSD-based features.\n",
    "    seg: data slice of type [times, count rates, count rate error]^T\n",
    "    pcb: frequency bands to use for power colours\n",
    "    \"\"\"\n",
    "\n",
    "    times = seg[:,0]\n",
    "    dt = times[1:] - times[:-1]\n",
    "    dt = np.min(dt)\n",
    "\n",
    "    counts = seg[:,1]*dt\n",
    "    #ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
    "    freq, ps = make_psd(seg, navg=1)\n",
    "    #print(\"len(ps), before: \" + str(len(ps)))\n",
    "    if times[-1]-times[0] >= 2.*128.0:\n",
    "        tlen = (times[-1]-times[0])\n",
    "        nrebin = np.round(tlen/128.)\n",
    "        freq, ps = rebin_psd(freq, ps, n=nrebin, type='average')\n",
    "\n",
    "    #print(\"len(ps), after: \" + str(len(ps)))\n",
    "\n",
    "    freq = np.array(freq[1:])\n",
    "    ps = ps[1:]\n",
    "    #print(\"min(freq): \" + str(np.min(freq)))\n",
    "    #print(\"max(freq): \" + str(np.max(freq)))\n",
    "    #print(\"len(freq): \" + str(len(freq)))\n",
    "\n",
    "    binfreq, binps = total_psd(seg, 24)\n",
    "\n",
    "    #print(\"min(binps): \" + str(np.min(binps)))\n",
    "    #print(\"max(binps): \" + str(np.max(binps)))\n",
    "    fmax_ind = np.where(binps == np.max(binps))\n",
    "    #print(\"fmax_ind: \" + str(fmax_ind))\n",
    "    maxfreq = binfreq[fmax_ind[0]]\n",
    "    #print(\"maxfreq: \" + str(maxfreq))\n",
    "\n",
    "    ## find power in spectral bands for power-colours\n",
    "    pa_min_freq = freq.searchsorted(pcb[\"pa_min\"])\n",
    "    pa_max_freq = freq.searchsorted(pcb[\"pa_max\"])\n",
    "\n",
    "    pb_min_freq = freq.searchsorted(pcb[\"pb_min\"])\n",
    "    pb_max_freq = freq.searchsorted(pcb[\"pb_max\"])\n",
    "\n",
    "    pc_min_freq = freq.searchsorted(pcb[\"pc_min\"])\n",
    "    pc_max_freq = freq.searchsorted(pcb[\"pc_max\"])\n",
    "\n",
    "    pd_min_freq = freq.searchsorted(pcb[\"pd_min\"])\n",
    "    pd_max_freq = freq.searchsorted(pcb[\"pd_max\"])\n",
    "\n",
    "    psd_a = np.sum(ps[pa_min_freq:pa_max_freq])\n",
    "    psd_b = np.sum(ps[pb_min_freq:pb_max_freq])\n",
    "    psd_c = np.sum(ps[pc_min_freq:pc_max_freq])\n",
    "    psd_d = np.sum(ps[pd_min_freq:pd_max_freq])\n",
    "    pc1 = np.sum(ps[pc_min_freq:pc_max_freq])/np.sum(ps[pa_min_freq:pa_max_freq])\n",
    "    pc2 = np.sum(ps[pb_min_freq:pb_max_freq])/np.sum(ps[pd_min_freq:pd_max_freq])\n",
    "\n",
    "    return maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2\n",
    "\n",
    "def make_psd(segment, navg=1):\n",
    "\n",
    "    times = segment[:,0]\n",
    "    dt = times[1:] - times[:-1]\n",
    "    dt = np.min(dt)\n",
    "\n",
    "    counts = segment[:,1]*dt\n",
    "\n",
    "    tseg = times[-1]-times[0]\n",
    "    nlc = len(times)\n",
    "    nseg = int(nlc/navg)\n",
    "\n",
    "    if navg == 1:\n",
    "        ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
    "        ps.freq = np.array(ps.freq)\n",
    "        ps.ps = np.array(ps.ps)*ps.freq\n",
    "        return ps.freq, ps.ps\n",
    "    else:\n",
    "        ps_all = []\n",
    "        for n in xrange(navg):\n",
    "            t_small = times[n*nseg:(n+1)*nseg]\n",
    "            c_small = counts[n*nseg:(n+1)*nseg]\n",
    "            ps = powerspectrum.PowerSpectrum(t_small, counts=c_small, norm=\"rms\")\n",
    "            ps.freq = np.array(ps.freq)\n",
    "            ps.ps = np.array(ps.ps)*ps.freq\n",
    "            ps_all.append(ps.ps)\n",
    "\n",
    "        #print(np.array(ps_all).shape)\n",
    "\n",
    "        ps_all = np.average(np.array(ps_all), axis=0)\n",
    "\n",
    "        #print(ps_all.shape)\n",
    "\n",
    "    return ps.freq, ps_all\n",
    "\n",
    "epsilon = 1.e-8\n",
    "\n",
    "def total_psd(seg, bins):\n",
    "    times = seg[:,0]\n",
    "    dt = times[1:] - times[:-1]\n",
    "    dt = np.min(dt)\n",
    "    counts = seg[:,1]*dt\n",
    "\n",
    "    ps = powerspectrum.PowerSpectrum(times, counts=counts, norm=\"rms\")\n",
    "    ps.ps = np.array(ps.freq)*np.array(ps.ps)\n",
    "    binfreq = np.logspace(np.log10(ps.freq[1]-epsilon), np.log10(ps.freq[-1]+epsilon), bins)\n",
    "    #print(\"freq: \" + str(ps.freq[1:10]))\n",
    "    #print(\"binfreq: \" + str(binfreq[:10]))\n",
    "    binps, bin_edges, binno = scipy.stats.binned_statistic(ps.freq[1:], ps.ps[1:], statistic=\"mean\", bins=binfreq)\n",
    "    df = binfreq[1:]-binfreq[:-1]\n",
    "    binfreq = binfreq[:-1]+df/2.\n",
    "\n",
    "    return np.array(binfreq), np.array(binps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Energy Domain Features\n",
    "\n",
    "Hardness ratios, the ratio of the flux in one photon energy band versus the flux in another, is a typical and useful proxy for more detailed energy spectral modelling. X-ray binaries are well known for their changing energy spectra in different stages of outburst, and GRS 1915+105 is no exception. Any features relating to hardness ratios are useful not only because they are likely to be discriminative, but also because they have a direct link to physical processes in the source, and are directly interpretable to physicists. \n",
    "\n",
    "The *Rossi* X-ray Timing Explorer (RXTE for short) covers the energy range between 2 and 70 keV. Our data is split into four bands, one covering the whole range, and three that covers the lower range, midrange and upper range of the energy bands (need numbers here!). Hardness ratios are generally derived by dividing the flux in the midrange and upper range by the flux in the lower energy band. These are called hardness ratio 1 and 2, respectively (need to add exact definitions). \n",
    "In general, a scatter plot of one hardness ratio against another (an X-ray colour-colour diagram) for a time series reveals that in different states, hardness ratios can differ by a facter of 10 or more. In some states, the source also makes a certain track in the colour-colour diagram. \n",
    "\n",
    "Our task is to summarize this behaviour in as few features as possible. \n",
    "The approach we choose here is particularly simple: we compute means of both hardness ratios for each segment, as well as the covariance between the two. This gives us a total of five numbers: two means, two variances (diagonal of the covariance matrix) and one covariance (the other is symmetric to the first).\n",
    "This will likely **not** capture the full behaviour, because for some states, the colour-colour diagram isn't particularly well modelled by a 2D Gaussian, but our hope is that it will be good enough for the classification tasks attempted here. We also extract skewness and kurtosis for both hardness ratios, which we hope will capture some of the non-Gaussianities in the hardness ratios.\n",
    "\n",
    "We can also experiment with more complex representations (for example a 2D histogram representation) or a bivariate Gaussian distribution fit, but initial experiments suggest that these representations do not particularly increase recall or precision in classification tasks.\n",
    "\n",
    "The code is still here for completeness, as is a function that computes a 2D histogram of the hardness-intensity diagram (HID), another diagnostic often used in X-ray binary research, but that does not seem to add anything to classification tasks, either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_hrlimits(hr1, hr2):\n",
    "    \"\"\"\n",
    "    Limits on the hardness ratios based on all segments.\n",
    "    Used to generate the ranges for the 2-D histograms.\n",
    "    \"\"\"\n",
    "    \n",
    "    min_hr1 = np.min(hr1)\n",
    "    max_hr1 = np.max(hr1)\n",
    "\n",
    "    min_hr2 = np.min(hr2)\n",
    "    max_hr2 = np.max(hr2)\n",
    "    return [[min_hr1, max_hr1], [min_hr2, max_hr2]]\n",
    "\n",
    "def hr_maps(seg, bins=30, hrlimits=None):\n",
    "    times = seg[:,0]\n",
    "    counts = seg[:,1]\n",
    "    low_counts = seg[:,2]\n",
    "    mid_counts = seg[:,3]\n",
    "    high_counts = seg[:,4]\n",
    "    hr1 = np.log(mid_counts/low_counts)\n",
    "    hr2 = np.log(high_counts/low_counts)\n",
    "\n",
    "    if hrlimits is None:\n",
    "        hr_limits = compute_hrlimits(hr1, hr2)\n",
    "    else:\n",
    "        hr_limits = hrlimits\n",
    "\n",
    "    h, xedges, yedges = np.histogram2d(hr1, hr2, bins=bins,\n",
    "                                       range=hr_limits)\n",
    "    h = np.rot90(h)\n",
    "    h = np.flipud(h)\n",
    "    hmax = np.max(h)\n",
    "    #print(hmax)\n",
    "    hmask = np.where(h > hmax/20.)\n",
    "    hmask1 = np.where(h < hmax/20.)\n",
    "    hnew = copy.copy(h)\n",
    "    hnew[hmask[0], hmask[1]] = 1.\n",
    "    hnew[hmask1[0], hmask1[1]] = 0.0\n",
    "    return xedges, yedges, hnew\n",
    "\n",
    "def hr_fitting(seg):\n",
    "    counts = seg[:,1]\n",
    "    low_counts = seg[:,2]\n",
    "    mid_counts = seg[:,3]\n",
    "    high_counts = seg[:,4]\n",
    "    hr1 = mid_counts/low_counts\n",
    "    hr2 = high_counts/low_counts\n",
    "\n",
    "    # sometimes, low_counts is zero, which leads to \n",
    "    # infinite values in HR1 or HR2, which we will remove\n",
    "    hr1_mask = np.where(np.isfinite(hr1))\n",
    "    hr1 = hr1[hr1_mask]\n",
    "    hr2 = hr2[hr1_mask]\n",
    "\n",
    "    hr2_mask = np.where(np.isfinite(hr2))\n",
    "    hr1 = hr1[hr2_mask]\n",
    "    hr2 = hr2[hr2_mask]\n",
    "\n",
    "    mu1 = np.mean(hr1)\n",
    "    mu2 = np.mean(hr2)\n",
    "\n",
    "    cov = np.cov(hr1, hr2).flatten()\n",
    "\n",
    "    skew = scipy.stats.skew(np.array([hr1, hr2]).T)\n",
    "    kurt = scipy.stats.kurtosis(np.array([hr1, hr2]).T)\n",
    "\n",
    "    if np.any(np.isnan(cov)):\n",
    "        print(\"NaN in cov\")\n",
    "\n",
    "    return mu1, mu2, cov, skew, kurt\n",
    "\n",
    "def hid_maps(seg, bins=30):\n",
    "    counts = seg[:,1]\n",
    "    low_counts = seg[:,2]\n",
    "    high_counts = seg[:,3]\n",
    "    hr2 = high_counts/low_counts\n",
    "    hid_limits = compute_hrlimits(hr2, counts)\n",
    "\n",
    "    h, xedges, yedges = np.histogram2d(hr2, counts, bins=bins,\n",
    "                                       range=hid_limits)\n",
    "    h = np.rot90(h)\n",
    "    h = np.flipud(h)\n",
    "\n",
    "    return xedges, yedges, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, for completeness, are another few snippets of code that extracts binned versions of the light curve in case I ever want to plug these into a classifier (but I generally don't) as well as a piece of code that extracts the (1) full light curve, (2) hardness ratio 1 and (3) hardness ratio 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lcshape_features(seg, dt=1.0):\n",
    "    \n",
    "    times = seg[:,0]\n",
    "    counts = seg[:,1]\n",
    "\n",
    "    dt_small = times[1:]-times[:-1]\n",
    "    dt_small = np.min(dt_small)\n",
    "\n",
    "    nbins = np.round(dt/dt_small)\n",
    "\n",
    "    bintimes, bincounts = rebin_psd(times, counts, nbins)\n",
    "    \n",
    "    return bincounts\n",
    "\n",
    "def extract_lc(seg):\n",
    "    times = seg[:,0]\n",
    "    counts = seg[:,1]\n",
    "    low_counts = seg[:,2]\n",
    "    high_counts = seg[:,3]\n",
    "    hr1 = low_counts/counts\n",
    "    hr2 = high_counts/counts\n",
    "    return [times, counts, hr1, hr2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Making Features\n",
    "\n",
    "Now we can use this code to extract some features and put them into a giant vector.\n",
    "We will need to extract the time series features using the linear filter separately, because this involves scaling *all* light curves to unit mean and variance, and then using `LinearFilterEnsemble` on the entire thing. \n",
    "So, we'll first extract all other features, then we'll extract the features from the linear filter, and then we'll stack them all together. Easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_features(seg, k=10, bins=30, navg=4, hr_summary=True, ps_summary=True, lc=True, hr=True, hrlimits=None):\n",
    "    \"\"\"\n",
    "    Make features from a set of light curve segments, except for the linear filter!\n",
    "    \n",
    "    :param seg: list of all segments to be used\n",
    "    :param bins: bins used in a 2D histogram if hr_summary is False\n",
    "    :param hr_summary: if True, summarize HRs in means and covariance matrix\n",
    "    :param ps_summary: if True, summarize power spectrum in frequency of maximum power and power spectral bands\n",
    "    :param lc: if True, store light curves\n",
    "    :param hr: if True, store hardness ratios\n",
    "    :param hrlimits: limits for the 2D histogram if hr_summary is False\n",
    "    :return: fdict: dictionary with keywords \"features\", \"lc\" and \"hr\"\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    if lc:\n",
    "        lc_all = []\n",
    "    if hr:\n",
    "        hr_all = []\n",
    "        \n",
    "        \n",
    "    ## MAKE FEATURES BASED ON LINEAR FILTER\n",
    "\n",
    "    ## first, extract the total counts out of each segment\n",
    "    counts = np.array([s[:,1] for s in seg])\n",
    "\n",
    "    ## next, transform the array such that *each light curve* is scaled\n",
    "    ## to zero mean and unit variance\n",
    "    ## We can do this for all light curves independently, because we're\n",
    "    ## averaging *per light curve* and not *per time bin*\n",
    "    counts_scaled = StandardScaler().fit_transform(counts.T).T\n",
    "\n",
    "    ## transform the counts into a weight vector\n",
    "    ww = linear_filter(counts_scaled, k=k)\n",
    "    \n",
    "    for s in seg:\n",
    "\n",
    "        features_temp = []\n",
    "\n",
    "        ## time series summary features\n",
    "        fmean, fmedian, fvar, fskew, fkurt = timeseries_features(s)\n",
    "        features_temp.extend([fmean, fmedian, fvar, fskew, fkurt])\n",
    "\n",
    "        if ps_summary:\n",
    "            ## PSD summary features\n",
    "            maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2 = psd_features(s, pcb)\n",
    "            features_temp.extend([maxfreq, psd_a, psd_b, psd_c, psd_d, pc1, pc2])\n",
    "        else:\n",
    "            ## whole PSD\n",
    "            binfreq, binps = total_psd(s, 24)\n",
    "            features_temp.extend(binps[1:])\n",
    "\n",
    "        if hr_summary:\n",
    "            mu1, mu2, cov, skew, kurt = hr_fitting(s)\n",
    "            features_temp.extend([mu1, mu2])\n",
    "            features_temp.extend([cov[0], cov[1], cov[3]])\n",
    "            features_temp.extend(list(skew))\n",
    "            features_temp.extend(list(kurt))\n",
    "\n",
    "        else:\n",
    "            xedges, yedges, h = hr_maps(s, bins=bins, hrlimits=hrlimits)\n",
    "            features_temp.extend(h.flatten())\n",
    "\n",
    "        features.append(features_temp)\n",
    "\n",
    "        if lc or hr:\n",
    "            lc_temp = extract_lc(s)\n",
    "        if lc:\n",
    "            lc_all.append([lc_temp[0], lc_temp[1]])\n",
    "        if hr:\n",
    "            hr_all.append([lc_temp[2], lc_temp[3]])\n",
    "\n",
    "    features_all = np.hstack((np.array(features), ww))\n",
    "    \n",
    "    fdict = {\"features\": features_all}\n",
    "    if lc:\n",
    "        fdict[\"lc\"] = lc_all\n",
    "    if hr:\n",
    "        fdict[\"hr\"] = hr_all\n",
    "    return fdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make the dictionaries with features, light curves and hardness ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/daniela/sw/miniconda/lib/python2.7/site-packages/ipykernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "features_train = make_features(seg_train[:6], bins=30)\n",
    "features_val = make_features(seg_val[:4], bins=30)\n",
    "features_test = make_features(seg_test[:5], bins=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of the features is the following:\n",
    "\n",
    "\n",
    "1. mean\n",
    "2. median\n",
    "3. variance\n",
    "4. skewness\n",
    "5. kurtosis\n",
    "\n",
    "If `ps_summary` is set to `True`, then the list continues:\n",
    "\n",
    "6. frequency where the periodogram has its maximum power\n",
    "7. integrated power in PSD band A\n",
    "8. integrated power in PSD band B\n",
    "9. integrated power in PSD band C\n",
    "10. integrated power in PSD band D\n",
    "11. Power Colour 1: PSD C / PSD A\n",
    "12. Power Colour 2: PSD B / BSD D\n",
    "\n",
    "otherwise features 6-30 are the power in 24 power spectral bins.\n",
    "\n",
    "If `hr_summary` is set to `True`, then the list continues:\n",
    "\n",
    "13. mean of HR 1\n",
    "14. mean of HR 2\n",
    "15. variance of HR 1\n",
    "16. covariance between HR 1 and HR 2 \n",
    "17. variance of HR 2\n",
    "18. skewness of HR 1\n",
    "19. skewness of HR 2\n",
    "20. kurtosis of HR 1\n",
    "21. kurtosis of HR 2\n",
    "\n",
    "otherwise, the next set of features is an `(nbins x nbins)` long list\n",
    "of bins from a 2D histogram representation.\n",
    "\n",
    "The last `[-k:]` features contain the `k`-long weight vector for each \n",
    "light curve describing the linear filter.\n",
    "\n",
    "\n",
    "### Saving the data for future use\n",
    "\n",
    "Let's save our features in a bunch of files for future use. \n",
    "Because the feature matrices are not that big, they go into a simple ascii file.\n",
    "Light curves and hardness ratios go into pickle files. Let's define a little helper\n",
    "function to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Remember how we're doing the longer segments right now? \n",
    "## Yeah, don't forget that in the file name!\n",
    "\n",
    "def save_features(fdict, seg_length, lc=True, hr=True, froot=\"../../grs1915\"):\n",
    "    froot = froot + str(int(seg_length))\n",
    "    np.savetxt(froot + \"_features.txt\", fdict[\"features\"])\n",
    "    if lc:\n",
    "        f = open(froot + \"_lightcurves.dat\", \"w\")\n",
    "        pickle.dump(fdict[\"lc\"], f)\n",
    "        f.close()\n",
    "    if hr:\n",
    "        f = open(froot + \"_hardness.dat\", \"w\")\n",
    "        pickle.dump(fdict[\"hr\"], f)\n",
    "        f.close()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_features(features_train, seg_length_supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining it all\n",
    "\n",
    "The code above starts with a file called `grs1915_all_125ms.dat`. This file \n",
    "has the data extracted from their original fits files and separated observations\n",
    "into individual light curves split up by data gaps. \n",
    "\n",
    "First, we define a helper function that goes through each observation, finds data gaps and splits the data along these gaps into uneven segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import generaltools as gt\n",
    "\n",
    "def remove_gaps(d, state):\n",
    "    \"\"\"\n",
    "    Split light curves into segments without gaps\n",
    "    \"\"\"\n",
    "    dt_data = d[1:,0]-d[:-1,0]\n",
    "    dt_min = np.min(dt_data)\n",
    "    tol = dt_min*0.01\n",
    "    \n",
    "    ### split data with breaks\n",
    "    breaks = np.where(dt_data > dt_min+tol)[0]\n",
    "\n",
    "    d_all = []\n",
    "    \n",
    "    if len(breaks) == 0:\n",
    "        dtemp = d\n",
    "        d_all.append([dtemp, state])\n",
    "    else:\n",
    "        for i,b in enumerate(breaks):\n",
    "            if i == 0:\n",
    "                if b == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    dtemp = d[:b]\n",
    "\n",
    "            else:\n",
    "                dtemp = d[breaks[i-1]+1:b]\n",
    "\n",
    "            d_all.append([dtemp, state])\n",
    "\n",
    "        ## last segment\n",
    "        dtemp = d[b+1:]\n",
    "        d_all.append([dtemp, state])\n",
    "\n",
    "    return d_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also need a function that rebins the data into light curves with a coarser \n",
    "time resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## helper function that bins light curves\n",
    "def bin_lightcurve(dtemp, nbins):\n",
    "    tbinned_times, tbinned_counts = gt.rebin_lightcurve(dtemp[:,0], dtemp[:,1], n=nbins, type=\"average\")\n",
    "    lbinned_times, lbinned_counts = gt.rebin_lightcurve(dtemp[:,0], dtemp[:,2], n=nbins, type=\"average\")\n",
    "    hbinned_times, hbinned_counts = gt.rebin_lightcurve(dtemp[:,0], dtemp[:,3], n=nbins, type=\"average\")\n",
    "    dshort = np.transpose(np.array([tbinned_times, tbinned_counts, lbinned_counts, hbinned_counts]))\n",
    "    return dshort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, for whatever reason, `inf`s and `nan`s might sneak into our feature vectors. \n",
    "This will make the Standard Scaler (and various machine learning algorithms) very unhappy,\n",
    "so we'd like to remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_nan(features, labels, hr=True, lc=True):\n",
    "    inf_ind = []\n",
    "    fnew, lnew, tnew = [], [], []\n",
    "    if lc:\n",
    "        lcnew = []\n",
    "    if hr:\n",
    "        hrnew = []\n",
    "\n",
    "    for i,f in enumerate(features[\"features\"]):\n",
    "\n",
    "        try:\n",
    "            if any(np.isnan(f)):\n",
    "                print(\"NaN in sample row %i\"%i)\n",
    "                continue\n",
    "            elif any(np.isinf(f)):\n",
    "                print(\"inf sample row %i\"%i)\n",
    "                continue\n",
    "            else:\n",
    "                fnew.append(f)\n",
    "                lnew.append(labels[i])\n",
    "                tnew.append(features[\"tstart\"][i])\n",
    "                if lc:\n",
    "                    lcnew.append(features[\"lc\"][i])\n",
    "                if hr:\n",
    "                    hrnew.append(features[\"hr\"][i])\n",
    "        except ValueError:\n",
    "            raise Exception(\"This is breaking! Boo!\")\n",
    "    features_new = {\"features\":fnew, \"tstart\":tnew}\n",
    "    if lc:\n",
    "        features_new[\"lc\"] = lcnew\n",
    "    if hr:\n",
    "        features_new[\"hr\"] = hrnew\n",
    "    return features_new, lnew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function that actually does all the feature extraction all in one step. Note that for extracting different features, you'll need to change those in the function below. I might make that more robust in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import convert_belloni \n",
    "\n",
    "def make_all_features(d_all, k=10, lamb=0.1, val=True, train_frac=0.6, validation_frac=0.2, test_frac = 0.2,\n",
    "                  seg=True, seg_length=1024., overlap=64., dt=0.125,\n",
    "                  bins=30, navg=4, hr_summary=True, ps_summary=True, lc=True, hr=True,\n",
    "                  save_features=True, froot=\"grs1915\", seed=20160615):\n",
    "\n",
    "    ## Set the seed to I will always pick out the same light curves.\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    ## shuffle list of light curves\n",
    "    indices = np.arange(len(d_all))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    n_lcs = len(d_all)\n",
    "\n",
    "    ## let's pull out light curves for three data sets into different variables.\n",
    "    d_all_train = [d_all[i] for i in indices[:int(train_frac*n_lcs)]]\n",
    "    d_all_test = [d_all[i] for i in indices[int(train_frac*n_lcs):int((train_frac + test_frac)*n_lcs)]]\n",
    "\n",
    "    seg_train, labels_train = extract_segments(d_all_train, seg_length=seg_length, \n",
    "                                               overlap=overlap, dt=dt)\n",
    "    seg_test, labels_test = extract_segments(d_all_test, seg_length=seg_length, \n",
    "                                             overlap=overlap, dt=dt)\n",
    "\n",
    "    if val:\n",
    "        d_all_val = [d_all[i] for i in indices[int((train_frac + test_frac)*n_lcs):]]\n",
    "        seg_val, labels_val = extract_segments(d_all_val, seg_length=seg_length, \n",
    "                                               overlap=overlap, dt=dt)\n",
    "\n",
    "\n",
    "    ### hrlimits are derived from the data, in the GRS1915_DataVisualisation Notebook\n",
    "    hrlimits = [[-2.5, 1.5], [-3.0, 2.0]]\n",
    "\n",
    "    features_train = make_features(seg_train,k, bins, lamb, hr_summary, ps_summary, lc, hr, hrlimits=hrlimits)\n",
    "    features_test = make_features(seg_test,k, bins,  lamb,hr_summary, ps_summary, lc, hr, hrlimits=hrlimits)\n",
    "\n",
    "    features_train[\"tstart\"] = tstart_train\n",
    "    features_test[\"tstart\"] = tstart_test\n",
    "\n",
    "    ## check for NaN\n",
    "    print(\"Checking for NaN in the training set ...\")\n",
    "    features_train_checked, labels_train_checked = check_nan(features_train, labels_train, hr=hr, lc=lc)\n",
    "    print(\"Checking for NaN in the test set ...\")\n",
    "    features_test_checked, labels_test_checked = check_nan(features_test, labels_test, hr=hr, lc=lc)\n",
    "\n",
    "\n",
    "    labelled_features = {\"train\": [features_train_checked[\"features\"], labels_train_checked],\n",
    "                     \"test\": [features_test_checked[\"features\"], labels_test_checked]}\n",
    "\n",
    "    if val:\n",
    "        features_val = make_features(seg_val, k, bins, lamb, hr_summary, ps_summary, lc, hr, hrlimits=hrlimits)\n",
    "        features_val[\"tstart\"] = tstart_val\n",
    "        \n",
    "        print(\"Checking for NaN in the validation set ...\")\n",
    "        features_val_checked, labels_val_checked = check_nan(features_val, labels_val, hr=hr, lc=lc)\n",
    "\n",
    "        labelled_features[\"val\"] =  [features_val_checked[\"features\"], labels_val_checked],\n",
    "\n",
    "    if save_features:\n",
    "        np.savetxt(froot+\"%i_features_train.txt\"%int(seg_length), features_train_checked[\"features\"])\n",
    "        np.savetxt(froot+\"%i_features_test.txt\"%int(seg_length), features_test_checked[\"features\"])\n",
    "\n",
    "        np.savetxt(froot+\"%i_tstart_train.txt\"%int(seg_length), features_train_checked[\"tstart\"])\n",
    "        np.savetxt(froot+\"%i_tstart_test.txt\"%int(seg_length), features_test_checked[\"tstart\"])\n",
    "\n",
    "        ltrainfile = open(froot+\"%i_labels_train.txt\"%int(seg_length), \"w\")\n",
    "        for l in labels_train_checked:\n",
    "            ltrainfile.write(str(l) + \"\\n\")\n",
    "        ltrainfile.close()\n",
    "\n",
    "        ltestfile = open(froot+\"%i_labels_test.txt\"%int(seg_length), \"w\")\n",
    "        for l in labels_test_checked:\n",
    "            ltestfile.write(str(l) + \"\\n\")\n",
    "        ltestfile.close()\n",
    "\n",
    "\n",
    "        if val:\n",
    "            np.savetxt(froot+\"%i_features_val.txt\"%int(seg_length), features_val_checked[\"features\"])\n",
    "            np.savetxt(froot+\"%i_tstart_val.txt\"%int(seg_length), features_val_checked[\"tstart\"])\n",
    "\n",
    "            lvalfile = open(froot+\"%i_labels_val.txt\"%int(seg_length), \"w\")\n",
    "            for l in labels_val_checked:\n",
    "                lvalfile.write(str(l) + \"\\n\")\n",
    "            lvalfile.close()\n",
    "\n",
    "\n",
    "        if lc:\n",
    "            lc_all = {\"train\":features_train[\"lc\"], \"test\":features_test[\"lc\"]}\n",
    "            if val:\n",
    "                lc_all[\"val\"] = features_val[\"lc\"]\n",
    "\n",
    "            f = open(froot+\"%i_lc_all.dat\"%int(seg_length), \"w\")\n",
    "            pickle.dump(lc_all, f, -1)\n",
    "            f.close()\n",
    "\n",
    "        if hr:\n",
    "            hr_all = {\"train\":features_train[\"hr\"], \"test\":features_test[\"hr\"]}\n",
    "            if val:\n",
    "                hr_all[\"val\"] = features_val[\"hr\"]\n",
    "\n",
    "            f = open(froot+\"%i_hr_all.dat\"%int(seg_length), \"w\")\n",
    "            pickle.dump(hr_all, f, -1)\n",
    "            f.close()\n",
    "            \n",
    "    return labelled_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Script\n",
    "\n",
    "For the actual analysis, I have put the stuff above into a script `feature_extraction.py`. \n",
    "We will run it on segments with the following choices determined by cross validation and \n",
    "common sense (see also feature engineering notebook):\n",
    "\n",
    "1. segments are 1024 seconds long (for supervised classification) and 256 seconds for unsupervised classification\n",
    "2. the overlap between consecutive segments is 64 seconds\n",
    "3. the fraction of training data is 0.5, those of validation and test data 0.25\n",
    "4. the number of weights, $k=$ for the linear filter (cross validation)\n",
    "5. the regularization parameter for ridge regression in the linear filter, is $\\alpha=$ (cross validation)\n",
    "\n",
    "More to come!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 (looks good, but samples lost when removing NaN values!)\n",
    "6 (also looses samples during removing NaNs)\n",
    "20160516 (low val score)\n",
    "20160523 \n",
    "20160525 (low val score)\n",
    "20160528 (low val score)\n",
    "20160601 (doesn't match results)\n",
    "20160606 (so far most decent, but still mis-classifies omega state)\n",
    "20160607 (best so far; manages to classify all)\n",
    "20160608 (does not work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_extraction.py:128: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  segments.append(dtemp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of states in the training set is 17\n",
      "The number of states in the test set is 17\n",
      "The number of states in the validation set is 17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-2745956ba3cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m feature_extraction.extract_all(d_all, seg_length_all=[1024.], overlap=[256.],\n\u001b[0;32m      4\u001b[0m                 \u001b[0mval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_frac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_frac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_frac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 datadir=\"../../\", seed=20160612)\n\u001b[0m",
      "\u001b[1;32m/scratch/daniela/data/grs1915/BlackHoleMagic/code/feature_extraction.pyc\u001b[0m in \u001b[0;36mextract_all\u001b[1;34m(d_all, seg_length_all, overlap, val, train_frac, validation_frac, test_frac, k, lamb, n, n_components, seed, datadir)\u001b[0m\n\u001b[0;32m    856\u001b[0m                   \u001b[0mseg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m                   \u001b[0mhr_summary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps_summary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m                   save_features=True, froot=datadir+\"grs1915\", seed=seed)\n\u001b[0m\u001b[0;32m    859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/data/grs1915/BlackHoleMagic/code/feature_extraction.pyc\u001b[0m in \u001b[0;36mmake_all_features\u001b[1;34m(d_all, k, lamb, n, n_components, val, train_frac, validation_frac, test_frac, seg, seg_length, overlap, dt, bins, navg, hr_summary, ps_summary, lc, hr, save_features, froot, seed)\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m#              hr_summary=True, ps_summary=True,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m     \u001b[1;31m#              lc=True, hr=True, hrlimits=None, n_components=3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m     \u001b[0mfeatures_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhr_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhrlimits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m     \u001b[0mfeatures_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mhr_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhrlimits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/data/grs1915/BlackHoleMagic/code/feature_extraction.pyc\u001b[0m in \u001b[0;36mmake_features\u001b[1;34m(seg, k, bins, lamb, n, hr_summary, ps_summary, lc, hr, hrlimits, n_components)\u001b[0m\n\u001b[0;32m    593\u001b[0m     \u001b[1;31m#print(\"extracting weights from AR model ...\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m     \u001b[1;31m# weights of the autoregressive model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m     \u001b[0mww\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts_binned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlamb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpsd_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/data/grs1915/BlackHoleMagic/code/feature_extraction.pyc\u001b[0m in \u001b[0;36mlinear_filter\u001b[1;34m(counts_scaled, k, lamb)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;31m## loop through all light curves and compute the weight vector for each\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mww_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mww_all\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/repositories/LightEcho/code/linearfilter.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mww\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mww\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/repositories/LightEcho/code/linearfilter.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mrr_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlamb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mrr_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m## best-fit output weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/sw/miniconda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \"\"\"\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRidge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/sw/miniconda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    474\u001b[0m                 \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m                 return_intercept=False)\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_intercept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/sw/miniconda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36mridge_regression\u001b[1;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, random_state, return_n_iter, return_intercept)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                 \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_solve_cholesky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 \u001b[1;31m# use SVD solver if matrix is singular\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/sw/miniconda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc\u001b[0m in \u001b[0;36m_solve_cholesky\u001b[1;34m(X, y, alpha)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mn_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mXy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/daniela/sw/miniconda/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reload(feature_extraction)\n",
    "\n",
    "feature_extraction.extract_all(d_all, seg_length_all=[1024.], overlap=[256.],\n",
    "                val=True, train_frac=0.5, validation_frac = 0.25, test_frac = 0.25,\n",
    "                datadir=\"../../\", seed=20160615)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
